2021-05-07 10:18:51 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: spider_pro)
2021-05-07 10:18:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.4.6, Platform Windows-10-10.0.19041-SP0
2021-05-07 10:18:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'spider_pro',
 'CONCURRENT_REQUESTS_PER_IP': 5,
 'DEPTH_PRIORITY': 1,
 'DOWNLOAD_DELAY': 2,
 'DOWNLOAD_TIMEOUT': 10,
 'LOG_FILE': 'E:\\LC\\qztb\\ztx_spider\\spider_pro\\logs/info_2021_05_07.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'spider_pro.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 400, 403, 404],
 'RETRY_TIMES': 3,
 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',
 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',
 'SPIDER_MODULES': ['spider_pro.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2021-05-07 10:18:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']
2021-05-07 10:19:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-05-07 10:19:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 559, in connect
    sock = self._connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 615, in _connect
    raise err
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 603, in _connect
    sock.connect(socket_address)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\twisted\internet\defer.py", line 1445, in _inlineCallbacks
    result = current_context.run(g.send, result)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 87, in crawl
    self.engine = self._create_engine()
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 101, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 35, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\utils\misc.py", line 167, in create_instance
    instance = objcls.from_crawler(crawler, *args, **kwargs)
  File "E:\LC\qztb\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 31, in from_crawler
    return cls(area_id, logger, **settings)
  File "E:\LC\qztb\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 23, in __init__
    if not self.redis_bloom_client.exists(self.key) and self.enable:
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 1581, in exists
    return self.execute_command('EXISTS', *names)
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 898, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 1192, in get_connection
    connection.connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 563, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10060 connecting to 192.168.1.248:6379. 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2021-05-07 16:44:51 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: spider_pro)
2021-05-07 16:44:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.4.6, Platform Windows-10-10.0.19041-SP0
2021-05-07 16:44:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'spider_pro',
 'CONCURRENT_REQUESTS': 32,
 'CONCURRENT_REQUESTS_PER_IP': 5,
 'DEPTH_PRIORITY': 1,
 'DOWNLOAD_DELAY': 2,
 'DOWNLOAD_TIMEOUT': 10,
 'LOG_FILE': 'E:\\LC\\ztx_spider\\spider_pro\\logs/info_2021_05_07.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'spider_pro.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 400, 403, 404],
 'RETRY_TIMES': 3,
 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',
 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',
 'SPIDER_MODULES': ['spider_pro.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2021-05-07 16:44:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']
2021-05-07 16:45:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-05-07 16:45:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 559, in connect
    sock = self._connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 615, in _connect
    raise err
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 603, in _connect
    sock.connect(socket_address)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\twisted\internet\defer.py", line 1445, in _inlineCallbacks
    result = current_context.run(g.send, result)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 87, in crawl
    self.engine = self._create_engine()
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 101, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 35, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\utils\misc.py", line 167, in create_instance
    instance = objcls.from_crawler(crawler, *args, **kwargs)
  File "E:\LC\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 31, in from_crawler
    return cls(area_id, logger, **settings)
  File "E:\LC\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 23, in __init__
    if not self.redis_bloom_client.exists(self.key) and self.enable:
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 1581, in exists
    return self.execute_command('EXISTS', *names)
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 898, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 1192, in get_connection
    connection.connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 563, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10060 connecting to 114.67.84.76:6379. 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2021-05-07 16:47:22 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: spider_pro)
2021-05-07 16:47:22 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.4.6, Platform Windows-10-10.0.19041-SP0
2021-05-07 16:47:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'spider_pro',
 'CONCURRENT_REQUESTS': 32,
 'CONCURRENT_REQUESTS_PER_IP': 5,
 'DEPTH_PRIORITY': 1,
 'DOWNLOAD_DELAY': 2,
 'DOWNLOAD_TIMEOUT': 10,
 'LOG_FILE': 'E:\\LC\\ztx_spider\\spider_pro\\logs/info_2021_05_07.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'spider_pro.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 400, 403, 404],
 'RETRY_TIMES': 3,
 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',
 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',
 'SPIDER_MODULES': ['spider_pro.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2021-05-07 16:47:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']
2021-05-07 16:47:44 [twisted] CRITICAL: Unhandled error in Deferred:
2021-05-07 16:47:44 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 559, in connect
    sock = self._connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 615, in _connect
    raise err
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 603, in _connect
    sock.connect(socket_address)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\LC\ztx_lc\lib\site-packages\twisted\internet\defer.py", line 1445, in _inlineCallbacks
    result = current_context.run(g.send, result)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 87, in crawl
    self.engine = self._create_engine()
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\crawler.py", line 101, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\middleware.py", line 35, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "E:\LC\ztx_lc\lib\site-packages\scrapy\utils\misc.py", line 167, in create_instance
    instance = objcls.from_crawler(crawler, *args, **kwargs)
  File "E:\LC\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 31, in from_crawler
    return cls(area_id, logger, **settings)
  File "E:\LC\ztx_spider\spider_pro\middlewares\UrlDuplicateRemovalMiddleware.py", line 23, in __init__
    if not self.redis_bloom_client.exists(self.key) and self.enable:
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 1581, in exists
    return self.execute_command('EXISTS', *names)
  File "E:\LC\ztx_lc\lib\site-packages\redis\client.py", line 898, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 1192, in get_connection
    connection.connect()
  File "E:\LC\ztx_lc\lib\site-packages\redis\connection.py", line 563, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10060 connecting to 114.67.84.76:6379. 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
